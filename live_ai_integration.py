#!/usr/bin/env python3
"""
Live AI Integration for FL Demo System
Supports real-world federated learning with MCP server and LangGraph
"""

import asyncio
import json
import logging
import os
import sys
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional
import threading
import time

# Core FL imports
try:
    import torch
    import torch.nn as nn
    import torch.optim as optim
    import numpy as np
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    print("‚ö†Ô∏è PyTorch not available - using simulation mode")

# MCP and LangGraph integration
try:
    import openai
    import anthropic
    from langchain.llms import OpenAI
    from langchain.agents import AgentExecutor
    from langchain.memory import ConversationBufferMemory
    AI_LIBS_AVAILABLE = True
except ImportError:
    AI_LIBS_AVAILABLE = False
    print("‚ö†Ô∏è AI libraries not available - install with pip install openai anthropic langchain")

logger = logging.getLogger(__name__)

class LiveAIManager:
    """Manages live AI integrations for the FL system."""

    def __init__(self):
        self.api_keys = {}
        self.ai_clients = {}
        self.mcp_server = None
        self.langgraph_agent = None
        self.fl_coordinator = None

        # Load saved API keys
        self._load_api_keys()

    def _load_api_keys(self):
        """Load API keys from environment or config file."""
        config_file = Path.home() / ".uiota" / "ai_config.json"

        if config_file.exists():
            try:
                with open(config_file, 'r') as f:
                    self.api_keys = json.load(f)
                logger.info("‚úÖ API keys loaded from config file")
            except Exception as e:
                logger.error(f"‚ùå Failed to load API keys: {e}")

        # Also check environment variables
        env_keys = {
            'openai': os.getenv('OPENAI_API_KEY'),
            'anthropic': os.getenv('ANTHROPIC_API_KEY'),
            'huggingface': os.getenv('HUGGINGFACE_API_KEY'),
            'cohere': os.getenv('COHERE_API_KEY')
        }

        for service, key in env_keys.items():
            if key and service not in self.api_keys:
                self.api_keys[service] = key

    def save_api_keys(self):
        """Save API keys to config file."""
        config_dir = Path.home() / ".uiota"
        config_dir.mkdir(exist_ok=True)
        config_file = config_dir / "ai_config.json"

        try:
            with open(config_file, 'w') as f:
                json.dump(self.api_keys, f, indent=2)
            logger.info("‚úÖ API keys saved to config file")
        except Exception as e:
            logger.error(f"‚ùå Failed to save API keys: {e}")

    def set_api_key(self, service: str, api_key: str):
        """Set API key for a service."""
        self.api_keys[service] = api_key
        self.save_api_keys()
        self._initialize_ai_client(service)
        logger.info(f"‚úÖ API key set for {service}")

    def _initialize_ai_client(self, service: str):
        """Initialize AI client for a service."""
        if not AI_LIBS_AVAILABLE:
            logger.warning("‚ö†Ô∏è AI libraries not available")
            return

        api_key = self.api_keys.get(service)
        if not api_key:
            logger.warning(f"‚ö†Ô∏è No API key for {service}")
            return

        try:
            if service == 'openai':
                self.ai_clients['openai'] = openai.OpenAI(api_key=api_key)
            elif service == 'anthropic':
                self.ai_clients['anthropic'] = anthropic.Anthropic(api_key=api_key)
            # Add more services as needed

            logger.info(f"‚úÖ {service} client initialized")
        except Exception as e:
            logger.error(f"‚ùå Failed to initialize {service} client: {e}")

    def get_available_services(self) -> List[str]:
        """Get list of available AI services."""
        return list(self.api_keys.keys())

    async def query_ai(self, service: str, prompt: str, **kwargs) -> str:
        """Query an AI service with a prompt."""
        if service not in self.ai_clients:
            return f"‚ùå {service} not available"

        try:
            if service == 'openai':
                response = await self.ai_clients['openai'].chat.completions.create(
                    model=kwargs.get('model', 'gpt-3.5-turbo'),
                    messages=[{"role": "user", "content": prompt}],
                    max_tokens=kwargs.get('max_tokens', 500)
                )
                return response.choices[0].message.content

            elif service == 'anthropic':
                response = await self.ai_clients['anthropic'].messages.create(
                    model=kwargs.get('model', 'claude-3-sonnet-20240229'),
                    max_tokens=kwargs.get('max_tokens', 500),
                    messages=[{"role": "user", "content": prompt}]
                )
                return response.content[0].text

            return "‚ùå Service not implemented"

        except Exception as e:
            logger.error(f"‚ùå AI query failed for {service}: {e}")
            return f"‚ùå Error: {e}"

class RealWorldFLCoordinator:
    """Coordinates real-world federated learning with live AI."""

    def __init__(self, ai_manager: LiveAIManager):
        self.ai_manager = ai_manager
        self.clients = {}
        self.global_model = None
        self.training_config = {
            'rounds': 100,
            'min_clients': 2,
            'learning_rate': 0.01,
            'batch_size': 32,
            'dataset': 'mnist'  # Default dataset
        }
        self.current_round = 0
        self.training_active = False
        self.metrics_history = []

    def initialize_global_model(self, model_type: str = 'cnn'):
        """Initialize the global FL model."""
        if not TORCH_AVAILABLE:
            logger.warning("‚ö†Ô∏è PyTorch not available - using mock model")
            self.global_model = MockModel()
            return

        try:
            if model_type == 'cnn' and SimpleCNN:
                self.global_model = SimpleCNN()
            elif model_type == 'linear' and SimpleLinear:
                self.global_model = SimpleLinear()
            elif TORCH_AVAILABLE:
                raise ValueError(f"Unknown model type: {model_type}")
            else:
                self.global_model = MockModel()

            logger.info(f"‚úÖ Global {model_type} model initialized")
        except Exception as e:
            logger.error(f"‚ùå Failed to initialize model: {e}")
            self.global_model = MockModel()

    async def register_client(self, client_id: str, client_info: Dict[str, Any]):
        """Register a new FL client."""
        self.clients[client_id] = {
            'info': client_info,
            'status': 'registered',
            'last_update': datetime.now(),
            'performance': {'accuracy': 0.0, 'loss': float('inf')}
        }

        # Use AI to analyze client capabilities
        if self.ai_manager.get_available_services():
            service = self.ai_manager.get_available_services()[0]
            prompt = f"""
            Analyze this federated learning client and suggest optimal training parameters:

            Client Info: {json.dumps(client_info, indent=2)}

            Consider:
            - Hardware capabilities
            - Data distribution
            - Network conditions
            - Privacy requirements

            Provide recommendations for:
            - Local epochs
            - Batch size
            - Learning rate
            """

            analysis = await self.ai_manager.query_ai(service, prompt)
            self.clients[client_id]['ai_analysis'] = analysis

        logger.info(f"‚úÖ Client {client_id} registered")
        return True

    async def start_training_round(self):
        """Start a new federated learning training round."""
        if not self.training_active:
            return False

        active_clients = [cid for cid, client in self.clients.items()
                         if client['status'] == 'ready']

        if len(active_clients) < self.training_config['min_clients']:
            logger.warning(f"‚ö†Ô∏è Not enough clients: {len(active_clients)}/{self.training_config['min_clients']}")
            return False

        self.current_round += 1
        logger.info(f"üöÄ Starting training round {self.current_round}")

        # Send global model to clients
        model_state = self.get_model_state()

        tasks = []
        for client_id in active_clients:
            task = self.send_model_to_client(client_id, model_state)
            tasks.append(task)

        # Wait for client updates
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # Aggregate client updates
        valid_updates = [r for r in results if isinstance(r, dict)]

        if valid_updates:
            await self.aggregate_updates(valid_updates)

            # Record metrics
            metrics = await self.evaluate_global_model()
            self.metrics_history.append({
                'round': self.current_round,
                'timestamp': datetime.now().isoformat(),
                'metrics': metrics,
                'participating_clients': len(valid_updates)
            })

            # Use AI for insights
            await self.generate_ai_insights(metrics)

        return True

    async def send_model_to_client(self, client_id: str, model_state: Dict):
        """Send global model to a client for local training."""
        try:
            # Simulate client training
            await asyncio.sleep(1)  # Simulate network delay

            # Mock client update
            client_update = {
                'client_id': client_id,
                'model_update': model_state,  # In real FL, this would be gradients
                'local_metrics': {
                    'accuracy': 0.8 + np.random.random() * 0.15,
                    'loss': 0.1 + np.random.random() * 0.1,
                    'samples': np.random.randint(100, 1000)
                }
            }

            self.clients[client_id]['status'] = 'completed'
            self.clients[client_id]['last_update'] = datetime.now()

            return client_update

        except Exception as e:
            logger.error(f"‚ùå Failed to train client {client_id}: {e}")
            self.clients[client_id]['status'] = 'error'
            return None

    async def aggregate_updates(self, client_updates: List[Dict]):
        """Aggregate client model updates."""
        try:
            if TORCH_AVAILABLE and hasattr(self.global_model, 'parameters'):
                # Real FL aggregation (FedAvg)
                total_samples = sum(update['local_metrics']['samples'] for update in client_updates)

                # Weighted averaging based on sample size
                for param in self.global_model.parameters():
                    weighted_sum = torch.zeros_like(param)
                    for update in client_updates:
                        weight = update['local_metrics']['samples'] / total_samples
                        # In real FL, you'd use actual gradients here
                        weighted_sum += weight * torch.randn_like(param) * 0.01
                    param.data += weighted_sum

            logger.info(f"‚úÖ Aggregated {len(client_updates)} client updates")

        except Exception as e:
            logger.error(f"‚ùå Aggregation failed: {e}")

    async def evaluate_global_model(self) -> Dict[str, float]:
        """Evaluate the global model."""
        try:
            if TORCH_AVAILABLE and hasattr(self.global_model, 'eval'):
                self.global_model.eval()
                # In real FL, you'd evaluate on a test dataset
                accuracy = 0.85 + np.random.random() * 0.1
                loss = 0.2 - (self.current_round * 0.001)
            else:
                # Mock evaluation
                accuracy = min(0.95, 0.75 + self.current_round * 0.002)
                loss = max(0.01, 0.3 - self.current_round * 0.003)

            return {
                'accuracy': accuracy,
                'loss': loss,
                'f1_score': accuracy * 0.95,
                'precision': accuracy * 0.98,
                'recall': accuracy * 0.92
            }

        except Exception as e:
            logger.error(f"‚ùå Model evaluation failed: {e}")
            return {'accuracy': 0.0, 'loss': float('inf')}

    async def generate_ai_insights(self, metrics: Dict[str, float]):
        """Generate AI insights about training progress."""
        if not self.ai_manager.get_available_services():
            return

        try:
            service = self.ai_manager.get_available_services()[0]

            # Prepare context
            recent_metrics = self.metrics_history[-5:] if len(self.metrics_history) >= 5 else self.metrics_history

            prompt = f"""
            Analyze the federated learning training progress and provide insights:

            Current Round: {self.current_round}
            Current Metrics: {json.dumps(metrics, indent=2)}
            Recent History: {json.dumps(recent_metrics, indent=2)}

            Active Clients: {len([c for c in self.clients.values() if c['status'] == 'ready'])}
            Total Clients: {len(self.clients)}

            Please provide:
            1. Performance assessment
            2. Convergence analysis
            3. Recommendations for optimization
            4. Potential issues or concerns
            5. Next steps suggestions

            Keep response concise and actionable.
            """

            insights = await self.ai_manager.query_ai(service, prompt, max_tokens=300)

            # Store insights
            if self.metrics_history:
                self.metrics_history[-1]['ai_insights'] = insights

            logger.info("ü§ñ AI insights generated")

        except Exception as e:
            logger.error(f"‚ùå Failed to generate AI insights: {e}")

    def get_model_state(self) -> Dict:
        """Get current model state for distribution."""
        if TORCH_AVAILABLE and hasattr(self.global_model, 'state_dict'):
            return {'state_dict': str(self.global_model.state_dict())}
        else:
            return {'mock_state': f'round_{self.current_round}'}

    def get_training_status(self) -> Dict[str, Any]:
        """Get current training status."""
        return {
            'training_active': self.training_active,
            'current_round': self.current_round,
            'total_rounds': self.training_config['rounds'],
            'registered_clients': len(self.clients),
            'active_clients': len([c for c in self.clients.values() if c['status'] == 'ready']),
            'latest_metrics': self.metrics_history[-1] if self.metrics_history else None,
            'global_model_type': type(self.global_model).__name__ if self.global_model else None
        }

if TORCH_AVAILABLE:
    class SimpleCNN(nn.Module):
        """Simple CNN model for FL demonstration."""

        def __init__(self, num_classes=10):
            super(SimpleCNN, self).__init__()
            self.conv1 = nn.Conv2d(1, 32, 3, 1)
            self.conv2 = nn.Conv2d(32, 64, 3, 1)
            self.dropout1 = nn.Dropout2d(0.25)
            self.dropout2 = nn.Dropout2d(0.5)
            self.fc1 = nn.Linear(9216, 128)
            self.fc2 = nn.Linear(128, num_classes)

        def forward(self, x):
            x = self.conv1(x)
            x = torch.relu(x)
            x = self.conv2(x)
            x = torch.relu(x)
            x = torch.max_pool2d(x, 2)
            x = self.dropout1(x)
            x = torch.flatten(x, 1)
            x = self.fc1(x)
            x = torch.relu(x)
            x = self.dropout2(x)
            x = self.fc2(x)
            return torch.log_softmax(x, dim=1)

    class SimpleLinear(nn.Module):
        """Simple linear model for FL demonstration."""

        def __init__(self, input_size=784, num_classes=10):
            super(SimpleLinear, self).__init__()
            self.fc1 = nn.Linear(input_size, 256)
            self.fc2 = nn.Linear(256, 128)
            self.fc3 = nn.Linear(128, num_classes)

        def forward(self, x):
            x = torch.flatten(x, 1)
            x = torch.relu(self.fc1(x))
            x = torch.relu(self.fc2(x))
            x = self.fc3(x)
            return torch.log_softmax(x, dim=1)
else:
    # Create placeholder classes when PyTorch is not available
    SimpleCNN = None
    SimpleLinear = None

class MockModel:
    """Mock model when PyTorch is not available."""

    def __init__(self):
        self.parameters_count = 1000
        self.accuracy = 0.75

    def eval(self):
        pass

    def parameters(self):
        return []

class MCPServerIntegration:
    """MCP (Model Context Protocol) server integration."""

    def __init__(self, ai_manager: LiveAIManager, fl_coordinator: RealWorldFLCoordinator):
        self.ai_manager = ai_manager
        self.fl_coordinator = fl_coordinator
        self.server_running = False

    async def start_mcp_server(self, port: int = 3000):
        """Start the MCP server for AI model context sharing."""
        try:
            # MCP server implementation would go here
            # For now, we'll create a simple HTTP server that provides model context

            self.server_running = True
            logger.info(f"üåê MCP server started on port {port}")

            # Simulate MCP server functionality
            while self.server_running:
                await asyncio.sleep(5)

                # Broadcast model context to connected services
                context = await self.get_model_context()
                await self.broadcast_context(context)

        except Exception as e:
            logger.error(f"‚ùå MCP server error: {e}")
            self.server_running = False

    async def get_model_context(self) -> Dict[str, Any]:
        """Get current model context for sharing."""
        status = self.fl_coordinator.get_training_status()

        context = {
            'timestamp': datetime.now().isoformat(),
            'fl_status': status,
            'model_performance': status.get('latest_metrics', {}),
            'available_ai_services': self.ai_manager.get_available_services(),
            'context_type': 'federated_learning',
            'version': '1.0.0'
        }

        return context

    async def broadcast_context(self, context: Dict[str, Any]):
        """Broadcast model context to connected services."""
        # In a real implementation, this would send context to connected MCP clients
        logger.debug(f"üì° Broadcasting model context: {len(str(context))} bytes")

def create_live_ai_system():
    """Factory function to create the live AI system."""
    ai_manager = LiveAIManager()
    fl_coordinator = RealWorldFLCoordinator(ai_manager)
    mcp_server = MCPServerIntegration(ai_manager, fl_coordinator)

    return ai_manager, fl_coordinator, mcp_server

if __name__ == "__main__":
    # Demo usage
    async def main():
        ai_manager, fl_coordinator, mcp_server = create_live_ai_system()

        # Initialize FL system
        fl_coordinator.initialize_global_model('cnn')

        # Register demo clients
        await fl_coordinator.register_client('client_1', {
            'device_type': 'mobile',
            'data_size': 1000,
            'compute_power': 'low'
        })

        await fl_coordinator.register_client('client_2', {
            'device_type': 'desktop',
            'data_size': 5000,
            'compute_power': 'high'
        })

        # Start training
        fl_coordinator.training_active = True

        for _ in range(5):  # 5 training rounds
            success = await fl_coordinator.start_training_round()
            if success:
                status = fl_coordinator.get_training_status()
                print(f"Round {status['current_round']}: {status['latest_metrics']}")

            await asyncio.sleep(2)

    asyncio.run(main())